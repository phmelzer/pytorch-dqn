{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: DQN Implementations\n",
    "\n",
    "What's covered: \n",
    "\n",
    "* Explain fundamentals Deep-Q-Learning\n",
    "* Practical examples of Deep-Q-Learning\n",
    "    * CartPole example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Fundamentals](#1)\n",
    "    - [1.1 - Limitations of Q-Learning with Q-Tables](#1.1)\n",
    "    - [1.2 - Deep-Q-Learning](#1.2)\n",
    "    - [1.3 - Deep-Q-Network](#1.3)\n",
    "    - [1.4 - Experience Replay](#1.4)\n",
    "    - [1.5 - Target Network](#1.5)\n",
    "    - [1.2 - Deep-Q-Learning](#1.2)\n",
    "- [2 - Lunar Lander Example](#2)\n",
    "    - [2.1 - Packages](#2.1)\n",
    "    - [2.2 - Create environment with OpenAI Gym](#2.2)\n",
    "    - [2.3 - Take random actions in environment](#2.3)\n",
    "    - [2.4 - DQN Implementation from scratch with Tensorflow](#2.4)\n",
    "    - [2.5 - DQN implementation with Keras-RL](#2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bd797",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 - Limitations of Q-Learning with Q-Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fd97",
   "metadata": {},
   "source": [
    "The Q-learning algorithm do a pretty decent job in relatively small state spaces, but it's performance will drop-off considerably when we work in more complex and sophisticated environments. \n",
    "\n",
    "Think about a video game where a player has a large environment to roam around in. Each state in the environment would be represented by a set of pixels, and the agent may be able to take serveral actions from each state. The iterative process of computing and updating Q-values for each state-action pair in a large state space becomes computationally inefficient and perhaps infeasible due to the computational resources and time this may take.\n",
    "\n",
    "So what can we do when we want to manage more sophisticated environments with large state spaces? Well, rather than using value iteration to directly compute Q-values and find the optimal Q-function, we instead use a function approximation to estimate the optimal Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e0ad8",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## 1.2 - Deep-Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae141ae",
   "metadata": {},
   "source": [
    "We'll make use of a deep neural network to estimate the Q-values for each state-action pair in a given environment, and in turn, the network will approximate the optimal Q-function. The act of combining Q-learning with a deep neural network is called *Deep-Q-Learning*, and a deep neural network that approximates a Q-function is called *Deep-Q-Network* or *DQN*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a8870",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## 1.3 - Deep-Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883003f",
   "metadata": {},
   "source": [
    "Suppose we have some arbitrary deep neural network that accepts states from a given environment as input. For each given state input, the network outputs estimated Q-values for each action that can be taken from that state. The objective of this network is to approximate the optimal Q-function, and remember that the optimal Q-function will satisfy the Bellman equation.\n",
    "\n",
    "<img src=\"images/deep_q_network.png\" style=\"width:400;height:400px;\">\n",
    "<caption><center><font ><b>Figure 1</b>: Deep-Q-Network </center></caption>\n",
    "\n",
    "Which this in mind, the loss from the network ic calculated by comparing the outputted Q-values to the target Q-values from the right hand side of the Bellman equation, and as with any network, the objective here is to minmize this loss.\n",
    "    \n",
    "After the loss is calculated, the weights within the network are updated via SGD and backpropagation, again, just like with any other typical network. This process is done over and over again for each state in the environment until we sufficiently minimize the loss and get an approximate optimal Q-function.\n",
    "    \n",
    "**The Input**\n",
    "    \n",
    "The network accept states from the environment as input. In more complex environments, like a video games, images can be used as input. Usually there will be some preprocessing on these types of inputs. \n",
    "\n",
    "Sometimes a single frame is not enough to represent a single input state, so we have to stack a few consecutive frames to represent a single input. \n",
    "    \n",
    "**The Layers**\n",
    "\n",
    "The layers in a *Deep-Q-Network* are not different than layers in other known networks. Many *Deep-Q-Networks* are purely just some convolutional layers, followed by some non-linear activation function, and a couple of fully connected layers at the end. \n",
    "    \n",
    "**The Output**\n",
    "    \n",
    "The output layer is a fully connected layer and it produces the Q-value for each action that can be taken from the given state that was passed as input. There is no activation function after the output layer since we want the raw, non-transformed Q-values from the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a8af5",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## 1.4 - Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa05e62",
   "metadata": {},
   "source": [
    "With deep-q-networks, a technique called experience replay is often used during training. With this technique the agent's experience is stored at each time step in a data set called the *replay memory*. \n",
    "\n",
    "At time *t*, the agent's experience $e_{t}$ is defined as this tuple:\n",
    "\n",
    "$$e_{t}=(s_{t}, a_{t}, r_{t+1}, s_{t+1})$$\n",
    "\n",
    "All of the agent's experience at each time step over all episodes played by the agent are stored in the *replay memory*. In practice usually a finite size limit is set and only the last *N* experiences are stored. \n",
    "\n",
    "Why is the network trained by random samples from replay memory, rather than just providing the network with the sequential experiences as they occur in the environment? If the network learned only from consecutive samples of experience as they occured sequentially in the environment, the samples would be highly correlated and would therefore lead to inefficient and unstable learning. Taking random samples from replay memory breaks this correlation. \n",
    "\n",
    "**Training a Deep-Q-Network with Replay Memory**\n",
    "\n",
    "After storing an experiences in replay memory, a random batch of experiences is sampled from replay memory. The state is then passed to the network as input. The input state data then forward propagates through the network, using the same forward propagation technique like other general neural networks. The model then outputs an estimated Q-value for each possible action from the given input state. \n",
    "\n",
    "The loss is then calculated. This is done by comparing the Q-value output from the network for the action in the experience tuple and the corresponding optimal Q-value, or *target Q-value*, for the same action. Remember, the target Q-value is calculated using the expression from the right rand side of the Bellman equation. So, the loss is calculated by subtracting the Q-value for a given state-action pair from the optimal Q-value from the same state-action pair. \n",
    "\n",
    "To compute the optimal Q-value for any given state-action pair, the state *s'* is passed to the policy network, which will output the Q-values for each state-action pair using *s'* as the state and each of the possible next actions as *a'*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322d09e",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "## 1.5 - Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418ec4e",
   "metadata": {},
   "source": [
    "The target network is a second network that is used to calculate the target Q-values. Rather than calculate them from the policy network, they are obtained by a completely separate network, appropriately called the *target network*. \n",
    "\n",
    "The target network is a clone of the policy network. Its weights are frozen with the original policy network's weights, and are updated every certain amount of time steps. This certain amount of time steps can be looked at as yet another hyperparameter. As it turns out, the use of a target network removes much of the instability introduced by using only one network to calculate both the Q-values, as well as the target Q-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735367d9",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 - Lunar Lander Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce89dde",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "## 2.1 - The Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fc078",
   "metadata": {},
   "source": [
    "This environment is a classic rocket trajectory optimization problem. According to Pontryaginâ€™s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
    "\n",
    "<img src=\"images/lunar_lander.png\">\n",
    "<caption><center><font ><b>Figure 1</b>: Lunar Landar </center></caption>\n",
    "    \n",
    "**Action Space**\n",
    "\n",
    "There are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "    \n",
    "**Observation Space**\n",
    "    \n",
    "There are 8 states: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "    \n",
    "**Rewards**\n",
    "    \n",
    "Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points.\n",
    "    \n",
    "**Starting State**\n",
    "    \n",
    "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
    "    \n",
    "**Episode Termination**\n",
    "    \n",
    "The episode terminates if any one of the following occurs:\n",
    "* the lander crashes (the lander body gets in contact with the moon);\n",
    "* the lander gets outside of the viewport (x coordinate is greater than 1);\n",
    "* the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesnâ€™t move and doesnâ€™t collide with any other body: When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb3500",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "## 2.2 - Packages\n",
    "\n",
    "Let's first install and import all packages we need:\n",
    "- [numpy](https://www.numpy.org) - fundamental package for scientific computing with Python.\n",
    "- [tensorflow](https://www.tensorflow.org) - fundamental package for machine learning and artificial intelligence with Python.\n",
    "- [OpenAI gym](https://gym.openai.com) - toolkit for developing and comparing reinforcement learning algorithms. The gym library provides an easy-to-use suite of reinforcement learning tasks\n",
    "- [keras](https://keras.io/) - library for artificial neural networks and acts as an interface for the TensorFlow library.\n",
    "- [keras-rl2](https://keras-rl.readthedocs.io/en/latest/) - library for reinforcement learning agents.\n",
    "\n",
    "Lunar Landar documentation: [Lunar Lander Documentation](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)\n",
    "\n",
    "Available environments: [OpenAI Gym environments](https://www.gymlibrary.ml/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78601dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Install packages \"\"\"\n",
    "!pip install gym\n",
    "!pip install pygame\n",
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b31cb",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "## 2.3 - Take random actions in enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29562691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, label=\"1\")\n",
    "    ax2 = fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "    \n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"C0\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"C0\")\n",
    "    \n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "        \n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel(\"Score\", color=\"C1\")\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ax2.tick_params(axis=\"y\", colors=\"C1\")\n",
    "    \n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "    \n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f022679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 15:03:02.869532: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-26 15:03:02.872644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dfki.uni-bremen.de/pmelzer/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-07-26 15:03:02.872652: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/dfki.uni-bremen.de/pmelzer/anaconda3/envs/reinforcement_learning/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1 score -118.22 average_score -118.22\n",
      "episode:  2 score -156.81 average_score -137.51\n",
      "episode:  3 score -231.49 average_score -168.84\n",
      "episode:  4 score -90.14 average_score -149.16\n",
      "episode:  5 score -315.84 average_score -182.50\n",
      "episode:  6 score -148.24 average_score -176.79\n",
      "episode:  7 score -524.41 average_score -226.45\n",
      "episode:  8 score -99.96 average_score -210.64\n",
      "episode:  9 score -92.31 average_score -197.49\n",
      "episode:  10 score -306.18 average_score -208.36\n",
      "episode:  11 score -332.30 average_score -219.63\n",
      "episode:  12 score -173.92 average_score -215.82\n",
      "episode:  13 score -386.21 average_score -228.92\n",
      "episode:  14 score -119.61 average_score -221.12\n",
      "episode:  15 score -178.19 average_score -218.26\n",
      "episode:  16 score -121.40 average_score -212.20\n",
      "episode:  17 score -156.52 average_score -208.93\n",
      "episode:  18 score -259.15 average_score -211.72\n",
      "episode:  19 score -521.82 average_score -228.04\n",
      "episode:  20 score -85.91 average_score -220.93\n",
      "episode:  21 score -97.70 average_score -215.06\n",
      "episode:  22 score -112.24 average_score -210.39\n",
      "episode:  23 score -124.39 average_score -206.65\n",
      "episode:  24 score -105.42 average_score -202.43\n",
      "episode:  25 score -85.53 average_score -197.76\n",
      "episode:  26 score 1.29 average_score -190.10\n",
      "episode:  27 score -335.50 average_score -195.49\n",
      "episode:  28 score -245.45 average_score -197.27\n",
      "episode:  29 score -127.76 average_score -194.87\n",
      "episode:  30 score -343.10 average_score -199.81\n",
      "episode:  31 score -138.63 average_score -197.84\n",
      "episode:  32 score -289.53 average_score -200.71\n",
      "episode:  33 score -397.83 average_score -206.68\n",
      "episode:  34 score -119.61 average_score -204.12\n",
      "episode:  35 score -103.48 average_score -201.24\n",
      "episode:  36 score -69.67 average_score -197.59\n",
      "episode:  37 score -118.12 average_score -195.44\n",
      "episode:  38 score -150.04 average_score -194.25\n",
      "episode:  39 score -260.79 average_score -195.95\n",
      "episode:  40 score -130.06 average_score -194.30\n",
      "episode:  41 score -118.43 average_score -192.45\n",
      "episode:  42 score -71.92 average_score -189.58\n",
      "episode:  43 score -79.23 average_score -187.02\n",
      "episode:  44 score -193.33 average_score -187.16\n",
      "episode:  45 score -315.67 average_score -190.02\n",
      "episode:  46 score -75.86 average_score -187.53\n",
      "episode:  47 score -66.15 average_score -184.95\n",
      "episode:  48 score -134.22 average_score -183.89\n",
      "episode:  49 score -138.43 average_score -182.97\n",
      "episode:  50 score -95.80 average_score -181.22\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "env = gym.make('LunarLander-v2')  # load gym environment\n",
    "\n",
    "episodes = 50  # define number of episodes\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0    \n",
    "    while not done:  # take steps in an episode until done \n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    scores.append(score)\n",
    "    eps_history.append(1.0)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode: ', i, 'score %.2f' % score,\n",
    "                'average_score %.2f' % avg_score)\n",
    "    \n",
    "filename = 'learning_curve_lunar_lander_random_actions.png'\n",
    "x = [i+1 for i in range(episodes)]\n",
    "plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416b75d",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 - DQN Implementation from scratch with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0d0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create ReplayMemory Class\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, max_size, input_dims):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), \n",
    "                                    dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims),\n",
    "                                dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8241016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create DeepQNetwork Class \"\"\"\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "import keras\n",
    "\n",
    "class DeepQNetwork(keras.Model):\n",
    "    def __init__(self, input_dims, n_actions, fc1_dims, fc2_dims):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(n_actions, activation=None)    \n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dabd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create DQNAgent Class \"\"\"\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, lr, gamma, n_actions, epsilon, batch_size,\n",
    "                input_dims, epsilon_dec=1e-3, epsilon_min=0.01,\n",
    "                replace_target=1000, mem_size=1000000, fname='dqn_model.h5'):\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = epsilon_dec\n",
    "        self.eps_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.replace_target = replace_target\n",
    "        self.model_file = fname\n",
    "        self.memory = ReplayMemory(mem_size, input_dims)   \n",
    "        self.policy_network = DeepQNetwork(input_dims, n_actions, fc1_dims=256, fc2_dims=256)\n",
    "        self.policy_network.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "        self.target_network = DeepQNetwork(input_dims, n_actions, fc1_dims=256, fc2_dims=256)\n",
    "        self.target_network.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n",
    "        \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = np.array([observation])\n",
    "            actions = self.policy_network.predict(state)\n",
    "            action = np.argmax(actions)\n",
    "        return action\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        self.target_network.set_weights(self.policy_network.get_weights())\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        q_policy_network = self.policy_network.predict(states)\n",
    "        q_target_network = self.policy_network.predict(states_)\n",
    "\n",
    "        q_target = np.copy(q_policy_network)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "\n",
    "        q_target[batch_index, actions] = rewards + self.gamma * np.max(q_target_network, axis=1)*dones\n",
    "\n",
    "        self.policy_network.train_on_batch(states, q_target)\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "                \n",
    "        #if self.memory.mem_cntr % self.replace_target == 0:\n",
    "        #    self.replace_target_network()\n",
    "\n",
    "    def save_model(self):\n",
    "        self.policy_network.save(self.model_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.policy_network = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119ff56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score -295.87 average_score -295.87 epsilon 0.93\n",
      "episode:  1 score -111.39 average_score -203.63 epsilon 0.87\n",
      "episode:  2 score -20.62 average_score -142.63 epsilon 0.78\n",
      "episode:  3 score -225.68 average_score -163.39 epsilon 0.69\n",
      "episode:  4 score -100.02 average_score -150.72 epsilon 0.56\n",
      "episode:  5 score -106.21 average_score -143.30 epsilon 0.31\n",
      "episode:  6 score -552.53 average_score -201.76 epsilon 0.20\n",
      "episode:  7 score -166.12 average_score -197.31 epsilon 0.01\n",
      "episode:  8 score -435.36 average_score -223.76 epsilon 0.01\n",
      "episode:  9 score -79.49 average_score -209.33 epsilon 0.01\n",
      "episode:  10 score -170.12 average_score -205.76 epsilon 0.01\n",
      "episode:  11 score -94.92 average_score -196.53 epsilon 0.01\n",
      "episode:  12 score -522.36 average_score -221.59 epsilon 0.01\n",
      "episode:  13 score -171.16 average_score -217.99 epsilon 0.01\n",
      "episode:  14 score -91.05 average_score -209.53 epsilon 0.01\n",
      "episode:  15 score -220.72 average_score -210.23 epsilon 0.01\n",
      "episode:  16 score -236.41 average_score -211.77 epsilon 0.01\n",
      "episode:  17 score -195.52 average_score -210.86 epsilon 0.01\n",
      "episode:  18 score -221.64 average_score -211.43 epsilon 0.01\n",
      "episode:  19 score -220.08 average_score -211.86 epsilon 0.01\n",
      "episode:  20 score -245.02 average_score -213.44 epsilon 0.01\n",
      "episode:  21 score -197.67 average_score -212.73 epsilon 0.01\n",
      "episode:  22 score -172.49 average_score -210.98 epsilon 0.01\n",
      "episode:  23 score -214.15 average_score -211.11 epsilon 0.01\n",
      "episode:  24 score -232.29 average_score -211.96 epsilon 0.01\n",
      "episode:  25 score -102.10 average_score -207.73 epsilon 0.01\n",
      "episode:  26 score -277.14 average_score -210.30 epsilon 0.01\n",
      "episode:  27 score -80.65 average_score -205.67 epsilon 0.01\n",
      "episode:  28 score -217.60 average_score -206.08 epsilon 0.01\n",
      "episode:  29 score -185.44 average_score -205.39 epsilon 0.01\n",
      "episode:  30 score -104.07 average_score -202.13 epsilon 0.01\n",
      "episode:  31 score -105.57 average_score -199.11 epsilon 0.01\n",
      "episode:  32 score -253.02 average_score -200.74 epsilon 0.01\n",
      "episode:  33 score -179.47 average_score -200.12 epsilon 0.01\n",
      "episode:  34 score -122.49 average_score -197.90 epsilon 0.01\n",
      "episode:  35 score -122.30 average_score -195.80 epsilon 0.01\n",
      "episode:  36 score -108.40 average_score -193.44 epsilon 0.01\n",
      "episode:  37 score -68.11 average_score -190.14 epsilon 0.01\n",
      "episode:  38 score -77.66 average_score -187.25 epsilon 0.01\n",
      "episode:  39 score 10.33 average_score -182.31 epsilon 0.01\n",
      "episode:  40 score -118.89 average_score -180.77 epsilon 0.01\n",
      "episode:  41 score -128.53 average_score -179.52 epsilon 0.01\n",
      "episode:  42 score -473.08 average_score -186.35 epsilon 0.01\n",
      "episode:  43 score -249.09 average_score -187.78 epsilon 0.01\n",
      "episode:  44 score -87.51 average_score -185.55 epsilon 0.01\n",
      "episode:  45 score -15.98 average_score -181.86 epsilon 0.01\n",
      "episode:  46 score -46.66 average_score -178.99 epsilon 0.01\n",
      "episode:  47 score -103.83 average_score -177.42 epsilon 0.01\n",
      "episode:  48 score -4.68 average_score -173.89 epsilon 0.01\n",
      "episode:  49 score -56.04 average_score -171.54 epsilon 0.01\n",
      "episode:  50 score -37.96 average_score -168.92 epsilon 0.01\n",
      "episode:  51 score 16.76 average_score -165.35 epsilon 0.01\n",
      "episode:  52 score -32.92 average_score -162.85 epsilon 0.01\n",
      "episode:  53 score -46.72 average_score -160.70 epsilon 0.01\n",
      "episode:  54 score -117.22 average_score -159.91 epsilon 0.01\n",
      "episode:  55 score -54.47 average_score -158.03 epsilon 0.01\n",
      "episode:  56 score -87.06 average_score -156.78 epsilon 0.01\n",
      "episode:  57 score -506.86 average_score -162.82 epsilon 0.01\n",
      "episode:  58 score -23.83 average_score -160.46 epsilon 0.01\n",
      "episode:  59 score -90.44 average_score -159.29 epsilon 0.01\n",
      "episode:  60 score -68.96 average_score -157.81 epsilon 0.01\n",
      "episode:  61 score -59.04 average_score -156.22 epsilon 0.01\n",
      "episode:  62 score -43.42 average_score -154.43 epsilon 0.01\n",
      "episode:  63 score 186.66 average_score -149.10 epsilon 0.01\n",
      "episode:  64 score 210.56 average_score -143.57 epsilon 0.01\n",
      "episode:  65 score 185.21 average_score -138.58 epsilon 0.01\n",
      "episode:  66 score 93.48 average_score -135.12 epsilon 0.01\n",
      "episode:  67 score -197.43 average_score -136.04 epsilon 0.01\n",
      "episode:  68 score 222.64 average_score -130.84 epsilon 0.01\n",
      "episode:  69 score 224.00 average_score -125.77 epsilon 0.01\n",
      "episode:  70 score 249.75 average_score -120.48 epsilon 0.01\n",
      "episode:  71 score -104.96 average_score -120.27 epsilon 0.01\n",
      "episode:  72 score -14.42 average_score -118.82 epsilon 0.01\n",
      "episode:  73 score -24.45 average_score -117.54 epsilon 0.01\n",
      "episode:  74 score -84.76 average_score -117.10 epsilon 0.01\n",
      "episode:  75 score 202.34 average_score -112.90 epsilon 0.01\n",
      "episode:  76 score 153.15 average_score -109.44 epsilon 0.01\n",
      "episode:  77 score 210.47 average_score -105.34 epsilon 0.01\n",
      "episode:  78 score 259.23 average_score -100.73 epsilon 0.01\n",
      "episode:  79 score 201.22 average_score -96.95 epsilon 0.01\n",
      "episode:  80 score -12.90 average_score -95.92 epsilon 0.01\n",
      "episode:  81 score -78.34 average_score -95.70 epsilon 0.01\n",
      "episode:  82 score -44.37 average_score -95.08 epsilon 0.01\n",
      "episode:  83 score 245.63 average_score -91.03 epsilon 0.01\n",
      "episode:  84 score -2.57 average_score -89.99 epsilon 0.01\n",
      "episode:  85 score -27.76 average_score -89.26 epsilon 0.01\n",
      "episode:  86 score 215.19 average_score -85.76 epsilon 0.01\n",
      "episode:  87 score -94.71 average_score -85.87 epsilon 0.01\n",
      "episode:  88 score 256.37 average_score -82.02 epsilon 0.01\n",
      "episode:  89 score 36.28 average_score -80.71 epsilon 0.01\n",
      "episode:  90 score 216.64 average_score -77.44 epsilon 0.01\n",
      "episode:  91 score 8.93 average_score -76.50 epsilon 0.01\n",
      "episode:  92 score 224.64 average_score -73.26 epsilon 0.01\n",
      "episode:  93 score 206.93 average_score -70.28 epsilon 0.01\n",
      "episode:  94 score -256.92 average_score -72.25 epsilon 0.01\n",
      "episode:  95 score 16.42 average_score -71.32 epsilon 0.01\n",
      "episode:  96 score -349.20 average_score -74.19 epsilon 0.01\n",
      "episode:  97 score -329.31 average_score -76.79 epsilon 0.01\n",
      "episode:  98 score -142.34 average_score -77.45 epsilon 0.01\n",
      "episode:  99 score -103.53 average_score -77.71 epsilon 0.01\n",
      "episode:  100 score 196.86 average_score -72.79 epsilon 0.01\n",
      "episode:  101 score 234.34 average_score -69.33 epsilon 0.01\n",
      "episode:  102 score 153.60 average_score -67.59 epsilon 0.01\n",
      "episode:  103 score 213.96 average_score -63.19 epsilon 0.01\n",
      "episode:  104 score 141.80 average_score -60.77 epsilon 0.01\n",
      "episode:  105 score 236.62 average_score -57.34 epsilon 0.01\n",
      "episode:  106 score 214.67 average_score -49.67 epsilon 0.01\n",
      "episode:  107 score -20.90 average_score -48.22 epsilon 0.01\n",
      "episode:  108 score 200.30 average_score -41.86 epsilon 0.01\n",
      "episode:  109 score -68.98 average_score -41.76 epsilon 0.01\n",
      "episode:  110 score 210.31 average_score -37.95 epsilon 0.01\n",
      "episode:  111 score -19.19 average_score -37.20 epsilon 0.01\n",
      "episode:  112 score 205.62 average_score -29.92 epsilon 0.01\n",
      "episode:  113 score 89.44 average_score -27.31 epsilon 0.01\n",
      "episode:  114 score 198.55 average_score -24.41 epsilon 0.01\n",
      "episode:  115 score 211.72 average_score -20.09 epsilon 0.01\n",
      "episode:  116 score 114.83 average_score -16.58 epsilon 0.01\n",
      "episode:  117 score 173.98 average_score -12.88 epsilon 0.01\n",
      "episode:  118 score 215.19 average_score -8.51 epsilon 0.01\n",
      "episode:  119 score 232.62 average_score -3.99 epsilon 0.01\n",
      "episode:  120 score 38.00 average_score -1.16 epsilon 0.01\n",
      "episode:  121 score -200.41 average_score -1.18 epsilon 0.01\n",
      "episode:  122 score 227.68 average_score 2.82 epsilon 0.01\n",
      "episode:  123 score 124.52 average_score 6.20 epsilon 0.01\n",
      "episode:  124 score 168.34 average_score 10.21 epsilon 0.01\n",
      "episode:  125 score -158.81 average_score 9.64 epsilon 0.01\n",
      "episode:  126 score 253.83 average_score 14.95 epsilon 0.01\n",
      "episode:  127 score 205.54 average_score 17.82 epsilon 0.01\n",
      "episode:  128 score 255.36 average_score 22.55 epsilon 0.01\n",
      "episode:  129 score 240.11 average_score 26.80 epsilon 0.01\n",
      "episode:  130 score -5.45 average_score 27.79 epsilon 0.01\n",
      "episode:  131 score 211.41 average_score 30.96 epsilon 0.01\n",
      "episode:  132 score 210.62 average_score 35.59 epsilon 0.01\n",
      "episode:  133 score -49.79 average_score 36.89 epsilon 0.01\n",
      "episode:  134 score -147.88 average_score 36.64 epsilon 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  135 score 177.17 average_score 39.63 epsilon 0.01\n",
      "episode:  136 score 168.38 average_score 42.40 epsilon 0.01\n",
      "episode:  137 score 163.64 average_score 44.72 epsilon 0.01\n",
      "episode:  138 score 221.93 average_score 47.71 epsilon 0.01\n",
      "episode:  139 score 249.43 average_score 50.10 epsilon 0.01\n",
      "episode:  140 score -148.38 average_score 49.81 epsilon 0.01\n",
      "episode:  141 score 196.61 average_score 53.06 epsilon 0.01\n",
      "episode:  142 score 208.76 average_score 59.88 epsilon 0.01\n",
      "episode:  143 score 236.13 average_score 64.73 epsilon 0.01\n",
      "episode:  144 score 203.30 average_score 67.64 epsilon 0.01\n",
      "episode:  145 score 184.83 average_score 69.65 epsilon 0.01\n",
      "episode:  146 score 142.91 average_score 71.54 epsilon 0.01\n",
      "episode:  147 score 253.19 average_score 75.11 epsilon 0.01\n",
      "episode:  148 score 234.42 average_score 77.50 epsilon 0.01\n",
      "episode:  149 score 213.38 average_score 80.20 epsilon 0.01\n",
      "episode:  150 score 179.48 average_score 82.37 epsilon 0.01\n",
      "episode:  151 score 241.11 average_score 84.62 epsilon 0.01\n",
      "episode:  152 score 245.78 average_score 87.40 epsilon 0.01\n",
      "episode:  153 score -123.41 average_score 86.64 epsilon 0.01\n",
      "episode:  154 score 228.38 average_score 90.09 epsilon 0.01\n",
      "episode:  155 score 36.72 average_score 91.00 epsilon 0.01\n",
      "episode:  156 score 227.34 average_score 94.15 epsilon 0.01\n",
      "episode:  157 score 205.70 average_score 101.27 epsilon 0.01\n",
      "episode:  158 score 232.96 average_score 103.84 epsilon 0.01\n",
      "episode:  159 score 191.86 average_score 106.66 epsilon 0.01\n",
      "episode:  160 score -54.19 average_score 106.81 epsilon 0.01\n",
      "episode:  161 score 164.48 average_score 109.05 epsilon 0.01\n",
      "episode:  162 score 142.49 average_score 110.91 epsilon 0.01\n",
      "episode:  163 score -202.12 average_score 107.02 epsilon 0.01\n",
      "episode:  164 score -108.31 average_score 103.83 epsilon 0.01\n",
      "episode:  165 score -221.75 average_score 99.76 epsilon 0.01\n",
      "episode:  166 score -246.78 average_score 96.36 epsilon 0.01\n",
      "episode:  167 score -82.68 average_score 97.50 epsilon 0.01\n",
      "episode:  168 score -236.86 average_score 92.91 epsilon 0.01\n",
      "episode:  169 score 286.38 average_score 93.53 epsilon 0.01\n",
      "episode:  170 score 19.59 average_score 91.23 epsilon 0.01\n",
      "episode:  171 score 226.82 average_score 94.55 epsilon 0.01\n",
      "episode:  172 score -42.15 average_score 94.27 epsilon 0.01\n",
      "episode:  173 score 197.36 average_score 96.49 epsilon 0.01\n",
      "episode:  174 score 237.62 average_score 99.71 epsilon 0.01\n",
      "episode:  175 score -24.43 average_score 97.45 epsilon 0.01\n",
      "episode:  176 score 216.06 average_score 98.08 epsilon 0.01\n",
      "episode:  177 score 234.24 average_score 98.31 epsilon 0.01\n",
      "episode:  178 score 228.41 average_score 98.00 epsilon 0.01\n",
      "episode:  179 score 186.19 average_score 97.85 epsilon 0.01\n",
      "episode:  180 score 6.03 average_score 98.04 epsilon 0.01\n",
      "episode:  181 score 230.56 average_score 101.13 epsilon 0.01\n",
      "episode:  182 score 250.24 average_score 104.08 epsilon 0.01\n",
      "episode:  183 score 224.41 average_score 103.87 epsilon 0.01\n",
      "episode:  184 score 55.25 average_score 104.44 epsilon 0.01\n",
      "episode:  185 score -63.99 average_score 104.08 epsilon 0.01\n",
      "episode:  186 score 212.26 average_score 104.05 epsilon 0.01\n",
      "episode:  187 score -364.24 average_score 101.36 epsilon 0.01\n",
      "episode:  188 score 249.58 average_score 101.29 epsilon 0.01\n",
      "episode:  189 score -58.06 average_score 100.35 epsilon 0.01\n",
      "episode:  190 score -105.97 average_score 97.12 epsilon 0.01\n",
      "episode:  191 score 266.43 average_score 99.70 epsilon 0.01\n",
      "episode:  192 score 218.96 average_score 99.64 epsilon 0.01\n",
      "episode:  193 score -265.23 average_score 94.92 epsilon 0.01\n",
      "episode:  194 score 272.74 average_score 100.21 epsilon 0.01\n",
      "episode:  195 score 268.01 average_score 102.73 epsilon 0.01\n",
      "episode:  196 score 174.15 average_score 107.96 epsilon 0.01\n",
      "episode:  197 score 175.40 average_score 113.01 epsilon 0.01\n",
      "episode:  198 score 245.42 average_score 116.89 epsilon 0.01\n",
      "episode:  199 score 213.57 average_score 120.06 epsilon 0.01\n",
      "episode:  200 score 219.26 average_score 120.28 epsilon 0.01\n",
      "episode:  201 score 151.90 average_score 119.46 epsilon 0.01\n",
      "episode:  202 score 211.50 average_score 120.04 epsilon 0.01\n",
      "episode:  203 score 260.26 average_score 120.50 epsilon 0.01\n",
      "episode:  204 score 177.75 average_score 120.86 epsilon 0.01\n",
      "episode:  205 score 270.07 average_score 121.19 epsilon 0.01\n",
      "episode:  206 score 183.20 average_score 120.88 epsilon 0.01\n",
      "episode:  207 score 240.71 average_score 123.50 epsilon 0.01\n",
      "episode:  208 score 249.77 average_score 123.99 epsilon 0.01\n",
      "episode:  209 score -8.38 average_score 124.60 epsilon 0.01\n",
      "episode:  210 score 254.85 average_score 125.04 epsilon 0.01\n",
      "episode:  211 score -24.91 average_score 124.99 epsilon 0.01\n",
      "episode:  212 score 241.78 average_score 125.35 epsilon 0.01\n",
      "episode:  213 score 256.78 average_score 127.02 epsilon 0.01\n",
      "episode:  214 score 260.58 average_score 127.64 epsilon 0.01\n",
      "episode:  215 score 246.63 average_score 127.99 epsilon 0.01\n",
      "episode:  216 score 172.81 average_score 128.57 epsilon 0.01\n",
      "episode:  217 score 186.65 average_score 128.70 epsilon 0.01\n",
      "episode:  218 score 249.53 average_score 129.04 epsilon 0.01\n",
      "episode:  219 score 190.47 average_score 128.62 epsilon 0.01\n",
      "episode:  220 score 225.83 average_score 130.50 epsilon 0.01\n",
      "episode:  221 score 182.86 average_score 134.33 epsilon 0.01\n",
      "episode:  222 score 224.75 average_score 134.30 epsilon 0.01\n",
      "episode:  223 score -27.03 average_score 132.78 epsilon 0.01\n",
      "episode:  224 score 256.27 average_score 133.66 epsilon 0.01\n",
      "episode:  225 score -123.08 average_score 134.02 epsilon 0.01\n",
      "episode:  226 score 213.16 average_score 133.61 epsilon 0.01\n",
      "episode:  227 score 188.59 average_score 133.44 epsilon 0.01\n",
      "episode:  228 score 6.84 average_score 130.96 epsilon 0.01\n",
      "episode:  229 score 251.21 average_score 131.07 epsilon 0.01\n",
      "episode:  230 score 239.21 average_score 133.52 epsilon 0.01\n",
      "episode:  231 score 187.65 average_score 133.28 epsilon 0.01\n",
      "episode:  232 score 241.02 average_score 133.58 epsilon 0.01\n",
      "episode:  233 score 256.48 average_score 136.65 epsilon 0.01\n",
      "episode:  234 score 273.94 average_score 140.86 epsilon 0.01\n",
      "episode:  235 score 194.42 average_score 141.04 epsilon 0.01\n",
      "episode:  236 score 228.28 average_score 141.64 epsilon 0.01\n",
      "episode:  237 score 197.93 average_score 141.98 epsilon 0.01\n",
      "episode:  238 score -199.32 average_score 137.77 epsilon 0.01\n",
      "episode:  239 score -162.10 average_score 133.65 epsilon 0.01\n",
      "episode:  240 score 132.85 average_score 136.46 epsilon 0.01\n",
      "episode:  241 score 213.63 average_score 136.63 epsilon 0.01\n",
      "episode:  242 score 193.20 average_score 136.48 epsilon 0.01\n",
      "episode:  243 score -68.14 average_score 133.44 epsilon 0.01\n",
      "episode:  244 score -4.99 average_score 131.35 epsilon 0.01\n",
      "episode:  245 score 254.32 average_score 132.05 epsilon 0.01\n",
      "episode:  246 score 244.59 average_score 133.06 epsilon 0.01\n",
      "episode:  247 score 232.44 average_score 132.86 epsilon 0.01\n",
      "episode:  248 score 222.53 average_score 132.74 epsilon 0.01\n",
      "episode:  249 score -174.75 average_score 128.86 epsilon 0.01\n",
      "episode:  250 score 236.47 average_score 129.43 epsilon 0.01\n",
      "episode:  251 score 231.34 average_score 129.33 epsilon 0.01\n",
      "episode:  252 score 226.50 average_score 129.14 epsilon 0.01\n",
      "episode:  253 score 193.30 average_score 132.30 epsilon 0.01\n",
      "episode:  254 score 229.40 average_score 132.31 epsilon 0.01\n",
      "episode:  255 score 276.76 average_score 134.71 epsilon 0.01\n",
      "episode:  256 score 251.29 average_score 134.95 epsilon 0.01\n",
      "episode:  257 score -45.21 average_score 132.44 epsilon 0.01\n",
      "episode:  258 score -176.20 average_score 128.35 epsilon 0.01\n",
      "episode:  259 score 268.74 average_score 129.12 epsilon 0.01\n",
      "episode:  260 score 302.55 average_score 132.69 epsilon 0.01\n",
      "episode:  261 score 147.98 average_score 132.52 epsilon 0.01\n",
      "episode:  262 score 285.95 average_score 133.96 epsilon 0.01\n",
      "episode:  263 score 250.82 average_score 138.49 epsilon 0.01\n",
      "episode:  264 score 241.78 average_score 141.99 epsilon 0.01\n",
      "episode:  265 score 134.76 average_score 145.55 epsilon 0.01\n",
      "episode:  266 score 229.88 average_score 150.32 epsilon 0.01\n",
      "episode:  267 score 202.15 average_score 153.17 epsilon 0.01\n",
      "episode:  268 score -63.02 average_score 154.91 epsilon 0.01\n",
      "episode:  269 score 231.35 average_score 154.36 epsilon 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  270 score 208.74 average_score 156.25 epsilon 0.01\n",
      "episode:  271 score 261.56 average_score 156.60 epsilon 0.01\n",
      "episode:  272 score 190.32 average_score 158.92 epsilon 0.01\n",
      "episode:  273 score 244.64 average_score 159.39 epsilon 0.01\n",
      "episode:  274 score 264.74 average_score 159.66 epsilon 0.01\n",
      "episode:  275 score 215.92 average_score 162.07 epsilon 0.01\n",
      "episode:  276 score -26.32 average_score 159.64 epsilon 0.01\n",
      "episode:  277 score 128.54 average_score 158.59 epsilon 0.01\n",
      "episode:  278 score 195.94 average_score 158.26 epsilon 0.01\n",
      "episode:  279 score -57.65 average_score 155.82 epsilon 0.01\n",
      "episode:  280 score 252.15 average_score 158.28 epsilon 0.01\n",
      "episode:  281 score 235.20 average_score 158.33 epsilon 0.01\n",
      "episode:  282 score 234.02 average_score 158.17 epsilon 0.01\n",
      "episode:  283 score -14.30 average_score 155.78 epsilon 0.01\n",
      "episode:  284 score 244.04 average_score 157.67 epsilon 0.01\n",
      "episode:  285 score -26.87 average_score 158.04 epsilon 0.01\n",
      "episode:  286 score 299.08 average_score 158.91 epsilon 0.01\n",
      "episode:  287 score 236.37 average_score 164.92 epsilon 0.01\n",
      "episode:  288 score -71.95 average_score 161.70 epsilon 0.01\n",
      "episode:  289 score 174.51 average_score 164.03 epsilon 0.01\n",
      "episode:  290 score 239.49 average_score 167.48 epsilon 0.01\n",
      "episode:  291 score 178.13 average_score 166.60 epsilon 0.01\n",
      "episode:  292 score 242.95 average_score 166.84 epsilon 0.01\n",
      "episode:  293 score 215.08 average_score 171.64 epsilon 0.01\n",
      "episode:  294 score 197.38 average_score 170.89 epsilon 0.01\n",
      "episode:  295 score 257.79 average_score 170.78 epsilon 0.01\n",
      "episode:  296 score -179.68 average_score 167.25 epsilon 0.01\n",
      "episode:  297 score 273.93 average_score 168.23 epsilon 0.01\n",
      "episode:  298 score 249.18 average_score 168.27 epsilon 0.01\n",
      "episode:  299 score -47.93 average_score 165.65 epsilon 0.01\n",
      "episode:  300 score 232.37 average_score 165.79 epsilon 0.01\n",
      "episode:  301 score 212.10 average_score 166.39 epsilon 0.01\n",
      "episode:  302 score 151.07 average_score 165.78 epsilon 0.01\n",
      "episode:  303 score 261.20 average_score 165.79 epsilon 0.01\n",
      "episode:  304 score 231.11 average_score 166.33 epsilon 0.01\n",
      "episode:  305 score 282.73 average_score 166.45 epsilon 0.01\n",
      "episode:  306 score 231.74 average_score 166.94 epsilon 0.01\n",
      "episode:  307 score 284.84 average_score 167.38 epsilon 0.01\n",
      "episode:  308 score 187.00 average_score 166.75 epsilon 0.01\n",
      "episode:  309 score 241.18 average_score 169.25 epsilon 0.01\n",
      "episode:  310 score 243.82 average_score 169.14 epsilon 0.01\n",
      "episode:  311 score 230.82 average_score 171.69 epsilon 0.01\n",
      "episode:  312 score 282.70 average_score 172.10 epsilon 0.01\n",
      "episode:  313 score 236.57 average_score 171.90 epsilon 0.01\n",
      "episode:  314 score 287.33 average_score 172.17 epsilon 0.01\n",
      "episode:  315 score 244.38 average_score 172.15 epsilon 0.01\n",
      "episode:  316 score 229.45 average_score 172.71 epsilon 0.01\n",
      "episode:  317 score 226.89 average_score 173.11 epsilon 0.01\n",
      "episode:  318 score 189.19 average_score 172.51 epsilon 0.01\n",
      "episode:  319 score 228.63 average_score 172.89 epsilon 0.01\n",
      "episode:  320 score 287.09 average_score 173.51 epsilon 0.01\n",
      "episode:  321 score -49.85 average_score 171.18 epsilon 0.01\n",
      "episode:  322 score 211.17 average_score 171.04 epsilon 0.01\n",
      "episode:  323 score -287.85 average_score 168.43 epsilon 0.01\n",
      "episode:  324 score 157.67 average_score 167.45 epsilon 0.01\n",
      "episode:  325 score 267.12 average_score 171.35 epsilon 0.01\n",
      "episode:  326 score 158.37 average_score 170.80 epsilon 0.01\n",
      "episode:  327 score 166.88 average_score 170.59 epsilon 0.01\n",
      "episode:  328 score 218.38 average_score 172.70 epsilon 0.01\n",
      "episode:  329 score 155.23 average_score 171.74 epsilon 0.01\n",
      "episode:  330 score 227.55 average_score 171.62 epsilon 0.01\n",
      "episode:  331 score 204.29 average_score 171.79 epsilon 0.01\n",
      "episode:  332 score 249.98 average_score 171.88 epsilon 0.01\n",
      "episode:  333 score 187.77 average_score 171.19 epsilon 0.01\n",
      "episode:  334 score 257.84 average_score 171.03 epsilon 0.01\n",
      "episode:  335 score 265.64 average_score 171.74 epsilon 0.01\n",
      "episode:  336 score 190.48 average_score 171.37 epsilon 0.01\n",
      "episode:  337 score 242.50 average_score 171.81 epsilon 0.01\n",
      "episode:  338 score -50.45 average_score 173.30 epsilon 0.01\n",
      "episode:  339 score -172.17 average_score 173.20 epsilon 0.01\n",
      "episode:  340 score 211.30 average_score 173.98 epsilon 0.01\n",
      "episode:  341 score 227.36 average_score 174.12 epsilon 0.01\n",
      "episode:  342 score 240.37 average_score 174.59 epsilon 0.01\n",
      "episode:  343 score 261.09 average_score 177.89 epsilon 0.01\n",
      "episode:  344 score -229.60 average_score 175.64 epsilon 0.01\n",
      "episode:  345 score -80.36 average_score 172.29 epsilon 0.01\n",
      "episode:  346 score -136.48 average_score 168.48 epsilon 0.01\n",
      "episode:  347 score -134.79 average_score 164.81 epsilon 0.01\n",
      "episode:  348 score -122.65 average_score 161.36 epsilon 0.01\n",
      "episode:  349 score 244.66 average_score 165.55 epsilon 0.01\n",
      "episode:  350 score 236.45 average_score 165.55 epsilon 0.01\n",
      "episode:  351 score 209.51 average_score 165.33 epsilon 0.01\n",
      "episode:  352 score 266.69 average_score 165.74 epsilon 0.01\n",
      "episode:  353 score 259.77 average_score 166.40 epsilon 0.01\n",
      "episode:  354 score 232.06 average_score 166.43 epsilon 0.01\n",
      "episode:  355 score 217.70 average_score 165.84 epsilon 0.01\n",
      "episode:  356 score 198.34 average_score 165.31 epsilon 0.01\n",
      "episode:  357 score 276.58 average_score 168.52 epsilon 0.01\n",
      "episode:  358 score -40.70 average_score 169.88 epsilon 0.01\n",
      "episode:  359 score -71.37 average_score 166.48 epsilon 0.01\n",
      "episode:  360 score 194.43 average_score 165.40 epsilon 0.01\n",
      "episode:  361 score 292.83 average_score 166.85 epsilon 0.01\n",
      "episode:  362 score 210.43 average_score 166.09 epsilon 0.01\n",
      "episode:  363 score 245.93 average_score 166.04 epsilon 0.01\n",
      "episode:  364 score 203.06 average_score 165.65 epsilon 0.01\n",
      "episode:  365 score 166.90 average_score 165.98 epsilon 0.01\n",
      "episode:  366 score 209.26 average_score 165.77 epsilon 0.01\n",
      "episode:  367 score 96.62 average_score 164.71 epsilon 0.01\n",
      "episode:  368 score 232.52 average_score 167.67 epsilon 0.01\n",
      "episode:  369 score 268.74 average_score 168.04 epsilon 0.01\n",
      "episode:  370 score -147.32 average_score 164.48 epsilon 0.01\n",
      "episode:  371 score 206.42 average_score 163.93 epsilon 0.01\n",
      "episode:  372 score 252.05 average_score 164.55 epsilon 0.01\n",
      "episode:  373 score -50.90 average_score 161.59 epsilon 0.01\n",
      "episode:  374 score 239.54 average_score 161.34 epsilon 0.01\n",
      "episode:  375 score 220.18 average_score 161.38 epsilon 0.01\n",
      "episode:  376 score 289.62 average_score 164.54 epsilon 0.01\n",
      "episode:  377 score 190.66 average_score 165.16 epsilon 0.01\n",
      "episode:  378 score 236.41 average_score 165.57 epsilon 0.01\n",
      "episode:  379 score 229.51 average_score 168.44 epsilon 0.01\n",
      "episode:  380 score -262.95 average_score 163.29 epsilon 0.01\n",
      "episode:  381 score 214.31 average_score 163.08 epsilon 0.01\n",
      "episode:  382 score -58.24 average_score 160.16 epsilon 0.01\n",
      "episode:  383 score -50.03 average_score 159.80 epsilon 0.01\n",
      "episode:  384 score 168.48 average_score 159.05 epsilon 0.01\n",
      "episode:  385 score 188.24 average_score 161.20 epsilon 0.01\n",
      "episode:  386 score 236.35 average_score 160.57 epsilon 0.01\n",
      "episode:  387 score -136.70 average_score 156.84 epsilon 0.01\n",
      "episode:  388 score 259.44 average_score 160.15 epsilon 0.01\n",
      "episode:  389 score -102.23 average_score 157.39 epsilon 0.01\n",
      "episode:  390 score 220.10 average_score 157.19 epsilon 0.01\n",
      "episode:  391 score 241.12 average_score 157.82 epsilon 0.01\n",
      "episode:  392 score -154.02 average_score 153.85 epsilon 0.01\n",
      "episode:  393 score 275.47 average_score 154.46 epsilon 0.01\n",
      "episode:  394 score 262.11 average_score 155.10 epsilon 0.01\n",
      "episode:  395 score 268.33 average_score 155.21 epsilon 0.01\n",
      "episode:  396 score 273.05 average_score 159.74 epsilon 0.01\n",
      "episode:  397 score 243.37 average_score 159.43 epsilon 0.01\n",
      "episode:  398 score 232.72 average_score 159.27 epsilon 0.01\n",
      "episode:  399 score 234.84 average_score 162.09 epsilon 0.01\n",
      "episode:  400 score 260.47 average_score 162.37 epsilon 0.01\n",
      "episode:  401 score 229.42 average_score 162.55 epsilon 0.01\n",
      "episode:  402 score 263.64 average_score 163.67 epsilon 0.01\n",
      "episode:  403 score 238.04 average_score 163.44 epsilon 0.01\n",
      "episode:  404 score 271.49 average_score 163.85 epsilon 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  405 score 262.26 average_score 163.64 epsilon 0.01\n",
      "episode:  406 score 240.96 average_score 163.73 epsilon 0.01\n",
      "episode:  407 score 253.85 average_score 163.42 epsilon 0.01\n",
      "episode:  408 score 231.55 average_score 163.87 epsilon 0.01\n",
      "episode:  409 score 278.61 average_score 164.24 epsilon 0.01\n",
      "episode:  410 score 257.36 average_score 164.38 epsilon 0.01\n",
      "episode:  411 score 276.39 average_score 164.83 epsilon 0.01\n",
      "episode:  412 score 266.27 average_score 164.67 epsilon 0.01\n",
      "episode:  413 score 272.21 average_score 165.03 epsilon 0.01\n",
      "episode:  414 score 226.45 average_score 164.42 epsilon 0.01\n",
      "episode:  415 score 213.50 average_score 164.11 epsilon 0.01\n",
      "episode:  416 score 223.54 average_score 164.05 epsilon 0.01\n",
      "episode:  417 score 241.27 average_score 164.19 epsilon 0.01\n",
      "episode:  418 score 223.83 average_score 164.54 epsilon 0.01\n",
      "episode:  419 score 277.20 average_score 165.02 epsilon 0.01\n",
      "episode:  420 score 256.93 average_score 164.72 epsilon 0.01\n",
      "episode:  421 score 217.65 average_score 167.40 epsilon 0.01\n",
      "episode:  422 score -211.68 average_score 163.17 epsilon 0.01\n",
      "episode:  423 score -182.78 average_score 164.22 epsilon 0.01\n",
      "episode:  424 score -212.99 average_score 160.51 epsilon 0.01\n",
      "episode:  425 score 177.42 average_score 159.62 epsilon 0.01\n",
      "episode:  426 score 238.30 average_score 160.42 epsilon 0.01\n",
      "episode:  427 score 186.44 average_score 160.61 epsilon 0.01\n",
      "episode:  428 score 244.79 average_score 160.88 epsilon 0.01\n",
      "episode:  429 score 258.99 average_score 161.91 epsilon 0.01\n",
      "episode:  430 score 281.70 average_score 162.46 epsilon 0.01\n",
      "episode:  431 score -34.63 average_score 160.07 epsilon 0.01\n",
      "episode:  432 score 247.75 average_score 160.04 epsilon 0.01\n",
      "episode:  433 score 246.76 average_score 160.63 epsilon 0.01\n",
      "episode:  434 score 291.89 average_score 160.97 epsilon 0.01\n",
      "episode:  435 score 3.52 average_score 158.35 epsilon 0.01\n",
      "episode:  436 score 260.76 average_score 159.06 epsilon 0.01\n",
      "episode:  437 score 290.88 average_score 159.54 epsilon 0.01\n",
      "episode:  438 score 287.37 average_score 162.92 epsilon 0.01\n",
      "episode:  439 score -45.74 average_score 164.18 epsilon 0.01\n",
      "episode:  440 score 269.66 average_score 164.77 epsilon 0.01\n",
      "episode:  441 score 260.89 average_score 165.10 epsilon 0.01\n",
      "episode:  442 score 219.89 average_score 164.90 epsilon 0.01\n",
      "episode:  443 score 205.69 average_score 164.34 epsilon 0.01\n",
      "episode:  444 score 282.62 average_score 169.46 epsilon 0.01\n",
      "episode:  445 score 184.24 average_score 172.11 epsilon 0.01\n",
      "episode:  446 score 218.29 average_score 175.66 epsilon 0.01\n",
      "episode:  447 score 244.00 average_score 179.45 epsilon 0.01\n",
      "episode:  448 score -85.76 average_score 179.81 epsilon 0.01\n",
      "episode:  449 score -40.63 average_score 176.96 epsilon 0.01\n",
      "episode:  450 score -196.86 average_score 172.63 epsilon 0.01\n",
      "episode:  451 score 214.97 average_score 172.68 epsilon 0.01\n",
      "episode:  452 score 221.55 average_score 172.23 epsilon 0.01\n",
      "episode:  453 score 91.61 average_score 170.55 epsilon 0.01\n",
      "episode:  454 score 230.56 average_score 170.54 epsilon 0.01\n",
      "episode:  455 score 100.03 average_score 169.36 epsilon 0.01\n",
      "episode:  456 score 258.04 average_score 169.96 epsilon 0.01\n",
      "episode:  457 score 52.05 average_score 167.71 epsilon 0.01\n",
      "episode:  458 score 210.78 average_score 170.23 epsilon 0.01\n",
      "episode:  459 score -423.89 average_score 166.70 epsilon 0.01\n",
      "episode:  460 score 297.92 average_score 167.73 epsilon 0.01\n",
      "episode:  461 score 277.47 average_score 167.58 epsilon 0.01\n",
      "episode:  462 score -96.50 average_score 164.51 epsilon 0.01\n",
      "episode:  463 score 289.68 average_score 164.95 epsilon 0.01\n",
      "episode:  464 score 249.44 average_score 165.41 epsilon 0.01\n",
      "episode:  465 score 218.30 average_score 165.93 epsilon 0.01\n",
      "episode:  466 score 193.86 average_score 165.77 epsilon 0.01\n",
      "episode:  467 score -14.58 average_score 164.66 epsilon 0.01\n",
      "episode:  468 score 252.98 average_score 164.87 epsilon 0.01\n",
      "episode:  469 score 212.58 average_score 164.30 epsilon 0.01\n",
      "episode:  470 score 209.68 average_score 167.87 epsilon 0.01\n",
      "episode:  471 score 283.34 average_score 168.64 epsilon 0.01\n",
      "episode:  472 score 205.19 average_score 168.18 epsilon 0.01\n",
      "episode:  473 score 141.80 average_score 170.10 epsilon 0.01\n",
      "episode:  474 score -496.04 average_score 162.75 epsilon 0.01\n",
      "episode:  475 score -332.93 average_score 157.22 epsilon 0.01\n",
      "episode:  476 score -498.15 average_score 149.34 epsilon 0.01\n",
      "episode:  477 score -357.87 average_score 143.85 epsilon 0.01\n",
      "episode:  478 score -694.75 average_score 134.54 epsilon 0.01\n",
      "episode:  479 score -1918.85 average_score 113.06 epsilon 0.01\n",
      "episode:  480 score -1073.90 average_score 104.95 epsilon 0.01\n",
      "episode:  481 score -725.84 average_score 95.55 epsilon 0.01\n",
      "episode:  482 score -658.53 average_score 89.54 epsilon 0.01\n",
      "episode:  483 score -1436.06 average_score 75.68 epsilon 0.01\n",
      "episode:  484 score -775.67 average_score 66.24 epsilon 0.01\n",
      "episode:  485 score -980.96 average_score 54.55 epsilon 0.01\n",
      "episode:  486 score -1624.76 average_score 35.94 epsilon 0.01\n",
      "episode:  487 score -612.26 average_score 31.18 epsilon 0.01\n",
      "episode:  488 score -584.18 average_score 22.75 epsilon 0.01\n",
      "episode:  489 score -459.85 average_score 19.17 epsilon 0.01\n",
      "episode:  490 score -597.97 average_score 10.99 epsilon 0.01\n",
      "episode:  491 score -341.67 average_score 5.16 epsilon 0.01\n",
      "episode:  492 score -352.71 average_score 3.17 epsilon 0.01\n",
      "episode:  493 score -311.65 average_score -2.70 epsilon 0.01\n",
      "episode:  494 score -316.78 average_score -8.49 epsilon 0.01\n",
      "episode:  495 score -158.83 average_score -12.76 epsilon 0.01\n",
      "episode:  496 score -182.88 average_score -17.32 epsilon 0.01\n",
      "episode:  497 score -325.21 average_score -23.00 epsilon 0.01\n",
      "episode:  498 score -562.03 average_score -30.95 epsilon 0.01\n",
      "episode:  499 score -421.59 average_score -37.51 epsilon 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Train Agent \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "'''\n",
    "In detail:-\n",
    "0 = all messages are logged (default behavior)\n",
    "1 = INFO messages are not printed\n",
    "2 = INFO and WARNING messages are not printed\n",
    "3 = INFO, WARNING, and ERROR messages are not printed\n",
    "'''\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "env = gym.make('LunarLander-v2')\n",
    "lr = 0.001\n",
    "episodes = 500\n",
    "agent = DQNAgent(gamma=0.99, epsilon=1.0, lr=lr, \n",
    "            input_dims=env.observation_space.shape,\n",
    "            n_actions=env.action_space.n, mem_size=1000000, batch_size=64,\n",
    "            epsilon_min=0.01)\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        agent.learn()\n",
    "    eps_history.append(agent.epsilon)\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode: ', i, 'score %.2f' % score,\n",
    "            'average_score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon)\n",
    "    \n",
    "filename = 'learning_curve_lunar_lander_dqn.png'\n",
    "x = [i+1 for i in range(episodes)]\n",
    "plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test trained agent\"\"\"\n",
    "\n",
    "episodes = 50\n",
    "scores = []\n",
    "eps_history = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    observation = env.reset()\n",
    "    agent.epsilon = 0.0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, _ = env.step(action)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "    eps_history.append(agent.epsilon)\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode: ', i, 'score %.2f' % score,\n",
    "            'average_score %.2f' % avg_score,\n",
    "            'epsilon %.2f' % agent.epsilon)\n",
    "\n",
    "filename = 'performance_lunar_lander_trained_dqn.png'\n",
    "x = [i+1 for i in range(episodes)]\n",
    "plotLearning(x, scores, eps_history, filename)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96bc67",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "## 2.5 - DQN implementation with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create instance of the model\n",
    "model = build_model(states, actions)\n",
    "\n",
    "# print model summary\n",
    "model.summary() \n",
    "'''\n",
    "\n",
    "\"\"\" Create DeepQNetwork Class \"\"\"\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "class DeepQNetwork(keras.Model):\n",
    "    def __init__(self, input_dims, n_actions, fc1_dims, fc2_dims):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(n_actions, activation=None)    \n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for tensorflow implementation\n",
    "import gym\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "model = DeepQNetwork(input_dims=env.observation_space.shape, n_actions=env.action_space.n, fc1_dims=256, fc2_dims=256)\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, policy=policy, nb_steps_warmup=10, target_model_update=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance \n",
    "dqn = build_agent(model, actions)\n",
    "# compile agent\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ddec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f176ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history[\"episode_reward\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8466a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd778b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "dqn.save_weights(\"dqn_weights.h5f\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete model, agent and environment to make sure the weights are deleted and then loaded\n",
    "del model\n",
    "del dqn\n",
    "del env\n",
    "\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all pieces again\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights to model\n",
    "dqn.load_weights(\"dqn_weights.h5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
