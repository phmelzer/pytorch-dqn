{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Fundamentals](#1)\n",
    "    - [1.1 - Characteristics](#1.1)\n",
    "    - [1.2 - Limitations of Q-Learning with Q-Tables](#1.1)\n",
    "    - [1.3 - Deep-Q-Learning](#1.2)\n",
    "    - [1.4 - Deep-Q-Network](#1.3)\n",
    "    - [1.5 - Experience Replay](#1.4)\n",
    "    - [1.6 - Target Network](#1.5)\n",
    "    - [1.7 - When to use Deep Q-Learning](#1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf104080",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 - Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b11779",
   "metadata": {},
   "source": [
    "Deep Q-Learning is a model-free, value-based, off-policy deep reinforcement learning algorithm which calculates updates according to the temporal difference method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bd797",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## 1.2 - Limitations of Q-Learning with Q-Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fd97",
   "metadata": {},
   "source": [
    "The Q-learning algorithm do a pretty decent job in relatively small state spaces, but it's performance will drop-off considerably when we work in more complex and sophisticated environments. \n",
    "\n",
    "Think about a video game where a player has a large environment to roam around in. Each state in the environment would be represented by a set of pixels, and the agent may be able to take serveral actions from each state. The iterative process of computing and updating Q-values for each state-action pair in a large state space becomes computationally inefficient and perhaps infeasible due to the computational resources and time this may take.\n",
    "\n",
    "So what can we do when we want to manage more sophisticated environments with large state spaces? Well, rather than using value iteration to directly compute Q-values and find the optimal Q-function, we instead use a function approximation to estimate the optimal Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e0ad8",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## 1.3 - Deep-Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae141ae",
   "metadata": {},
   "source": [
    "We'll make use of a deep neural network to estimate the Q-values for each state-action pair in a given environment, and in turn, the network will approximate the optimal Q-function. The act of combining Q-learning with a deep neural network is called *Deep-Q-Learning*, and a deep neural network that approximates a Q-function is called *Deep-Q-Network* or *DQN*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a8870",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## 1.4 - Deep-Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883003f",
   "metadata": {},
   "source": [
    "Suppose we have some arbitrary deep neural network that accepts states from a given environment as input. For each given state input, the network outputs estimated Q-values for each action that can be taken from that state. The objective of this network is to approximate the optimal Q-function, and remember that the optimal Q-function will satisfy the Bellman equation.\n",
    "\n",
    "<img src=\"images/deep_q_network.png\" style=\"width:400;height:400px;\">\n",
    "<caption><center><font ><b>Figure 1</b>: Deep-Q-Network </center></caption>\n",
    "\n",
    "Which this in mind, the loss from the network ic calculated by comparing the outputted Q-values to the target Q-values from the right hand side of the Bellman equation, and as with any network, the objective here is to minmize this loss.\n",
    "    \n",
    "After the loss is calculated, the weights within the network are updated via SGD and backpropagation, again, just like with any other typical network. This process is done over and over again for each state in the environment until we sufficiently minimize the loss and get an approximate optimal Q-function.\n",
    "    \n",
    "**The Input**\n",
    "    \n",
    "The network accept states from the environment as input. In more complex environments, like a video games, images can be used as input. Usually there will be some preprocessing on these types of inputs. \n",
    "\n",
    "Sometimes a single frame is not enough to represent a single input state, so we have to stack a few consecutive frames to represent a single input. \n",
    "    \n",
    "**The Layers**\n",
    "\n",
    "The layers in a *Deep-Q-Network* are not different than layers in other known networks. Many *Deep-Q-Networks* are purely just some convolutional layers, followed by some non-linear activation function, and a couple of fully connected layers at the end. \n",
    "    \n",
    "**The Output**\n",
    "    \n",
    "The output layer is a fully connected layer and it produces the Q-value for each action that can be taken from the given state that was passed as input. There is no activation function after the output layer since we want the raw, non-transformed Q-values from the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a8af5",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "## 1.5 - Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa05e62",
   "metadata": {},
   "source": [
    "With deep-q-networks, a technique called experience replay is often used during training. With this technique the agent's experience is stored at each time step in a data set called the *replay memory*. \n",
    "\n",
    "At time *t*, the agent's experience $e_{t}$ is defined as this tuple:\n",
    "\n",
    "$$e_{t}=(s_{t}, a_{t}, r_{t+1}, s_{t+1})$$\n",
    "\n",
    "All of the agent's experience at each time step over all episodes played by the agent are stored in the *replay memory*. In practice usually a finite size limit is set and only the last *N* experiences are stored. \n",
    "\n",
    "Why is the network trained by random samples from replay memory, rather than just providing the network with the sequential experiences as they occur in the environment? If the network learned only from consecutive samples of experience as they occured sequentially in the environment, the samples would be highly correlated and would therefore lead to inefficient and unstable learning. Taking random samples from replay memory breaks this correlation. \n",
    "\n",
    "**Training a Deep-Q-Network with Replay Memory**\n",
    "\n",
    "After storing an experiences in replay memory, a random batch of experiences is sampled from replay memory. The state is then passed to the network as input. The input state data then forward propagates through the network, using the same forward propagation technique like other general neural networks. The model then outputs an estimated Q-value for each possible action from the given input state. \n",
    "\n",
    "The loss is then calculated. This is done by comparing the Q-value output from the network for the action in the experience tuple and the corresponding optimal Q-value, or *target Q-value*, for the same action. Remember, the target Q-value is calculated using the expression from the right rand side of the Bellman equation. So, the loss is calculated by subtracting the Q-value for a given state-action pair from the optimal Q-value from the same state-action pair. \n",
    "\n",
    "To compute the optimal Q-value for any given state-action pair, the state *s'* is passed to the policy network, which will output the Q-values for each state-action pair using *s'* as the state and each of the possible next actions as *a'*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322d09e",
   "metadata": {},
   "source": [
    "<a name='1.6'></a>\n",
    "## 1.6 - Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418ec4e",
   "metadata": {},
   "source": [
    "The target network is a second network that is used to calculate the target Q-values. Rather than calculate them from the policy network, they are obtained by a completely separate network, appropriately called the *target network*. \n",
    "\n",
    "The target network is a clone of the policy network. Its weights are frozen with the original policy network's weights, and are updated every certain amount of time steps. This certain amount of time steps can be looked at as yet another hyperparameter. As it turns out, the use of a target network removes much of the instability introduced by using only one network to calculate both the Q-values, as well as the target Q-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f32df",
   "metadata": {},
   "source": [
    "<a name='1.7'></a>\n",
    "## 1.7 - When to use Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60866a",
   "metadata": {},
   "source": [
    "Deep Q-Learning should be used in single processes with discrete action spaces. To increase performance extensions (DDQN, Dueling DQN, DRQN, Prioritized experience replay) should be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
